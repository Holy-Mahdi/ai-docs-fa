# ⚡ گرادیان کاهشی (Gradient Descent)

## مقدمه
گرادیان کاهشی یک الگوریتم **تکراری** برای یافتن کمینه یک تابع هزینه است.  
ایده اصلی این است که پارامترها در **جهت عکس گرادیان** حرکت داده شوند تا مقدار تابع هزینه کاهش یابد.

:::note ایـده اصلی
- پارامترها با گام‌های کوچکی در جهت عکس گرادیان به‌روزرسانی می‌شوند.
- نرخ یادگیری (Learning Rate) اندازهٔ گام‌ها را تعیین می‌کند.
- الگوریتم تا رسیدن به حداقل یا تعداد مشخصی تکرار ادامه می‌یابد.
:::

## فرمول ریاضی

\[
\theta \leftarrow \theta - \alpha \nabla_\theta J(\theta)
\]

- $\theta$: بردار پارامترها  
- $\alpha$: نرخ یادگیری  
- $\nabla_\theta J(\theta)$: گرادیان تابع هزینه نسبت به پارامترها  

## پیاده‌سازی در پایتون
```python
import numpy as np

def gradient_descent(X, y, theta, lr=0.01, epochs=1000):
    """
    الگوریتم گرادیان کاهشی برای رگرسیون خطی
    X     : ماتریس ویژگی‌ها (m x n)
    y     : بردار خروجی‌ها (m,)
    theta : بردار پارامترها (n,)
    lr    : نرخ یادگیری
    epochs: تعداد تکرارها
    """
    m = len(y)
    for i in range(epochs):
        preds = X.dot(theta)        # پیش‌بینی با پارامترهای فعلی
        error = preds - y           # خطا
        grad = (1/m) * X.T.dot(error) # گرادیان
        theta -= lr * grad          # به‌روزرسانی پارامترها
    return theta
